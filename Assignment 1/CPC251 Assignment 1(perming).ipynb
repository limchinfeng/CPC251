{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6c8b0d1",
   "metadata": {},
   "source": [
    "#### Group Information\n",
    "\n",
    "Group No: Climate_3\n",
    "\n",
    "- Member 1: LIM CHIN FENG (157968)\n",
    "- Member 2: OOI YUE SHENG (158494)\n",
    "- Member 3: GWEE PER MING (159372)\n",
    "- Member 4: OOI YONG QIN (159067)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da74496",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43826fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi=False\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc916f",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4640766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          f1        f2        f3        f4        f5   response\n",
      "0  -0.764216 -1.016209  0.149410 -0.050119 -0.578127   6.242514\n",
      "1   0.763880 -1.159509 -0.721492 -0.654067 -0.431670  -8.118241\n",
      "2   0.519329 -0.664621 -1.694904  1.339779  0.182764  66.722455\n",
      "3  -0.177388  0.515623  0.135144 -0.647634 -0.405631 -27.716793\n",
      "4   0.104022  0.749665 -0.939338 -0.090725 -0.639963   8.192075\n",
      "5  -0.699867  0.019159  1.103377 -0.671614 -0.119063 -18.597563\n",
      "6  -1.028250  0.962967  0.471027 -1.941219 -0.465591 -73.174734\n",
      "7   0.337585  1.352948 -1.789795 -0.885796 -0.846150 -25.865464\n",
      "8   0.295433 -0.907789  0.275980 -0.675526 -0.942592  -9.001596\n",
      "9   0.442269 -0.704559 -1.127342  1.030206  0.800113  57.076963\n",
      "10 -0.204840 -1.806102 -1.047471 -0.026998  0.855309   8.256147\n",
      "11 -0.913785  0.812540 -0.142404  1.158025 -0.595457  36.095129\n",
      "12 -1.615746  1.026104 -0.358002 -1.082828 -0.634044 -43.717895\n",
      "13 -1.136888 -0.252588 -0.346081  0.829751 -0.786014  26.838977\n",
      "14  0.365964  0.217944  0.706191 -0.331472 -0.235984   2.830971\n",
      "15  0.460227 -0.881942  0.235676  0.910278 -1.100995  51.380414\n",
      "16 -0.770275  1.010911 -0.663917 -1.563364 -1.664088 -58.764946\n",
      "17 -1.797706 -0.051251 -1.773456 -0.623435 -0.253349 -39.743214\n",
      "18 -0.418531 -2.090379  0.781097 -1.481340 -0.225444 -48.189162\n",
      "19 -1.576976  1.151158 -1.134434  0.981570  0.254737  20.102213\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('assignment1_dataset.csv')\n",
    "print(dataset.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72caa77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates\n",
    "duplicate = dataset[dataset.duplicated()]\n",
    "print(dataset.duplicated().sum(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e2edcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   f1        1000 non-null   float64\n",
      " 1   f2        1000 non-null   float64\n",
      " 2   f3        1000 non-null   float64\n",
      " 3   f4        1000 non-null   float64\n",
      " 4   f5        1000 non-null   float64\n",
      " 5   response  1000 non-null   float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 47.0 KB\n",
      "\n",
      "Data Shape:  (1000, 6)\n",
      "         f1        f2        f3        f4        f5   response\n",
      "0 -0.764216 -1.016209  0.149410 -0.050119 -0.578127   6.242514\n",
      "1  0.763880 -1.159509 -0.721492 -0.654067 -0.431670  -8.118241\n",
      "2  0.519329 -0.664621 -1.694904  1.339779  0.182764  66.722455\n",
      "3 -0.177388  0.515623  0.135144 -0.647634 -0.405631 -27.716793\n",
      "4  0.104022  0.749665 -0.939338 -0.090725 -0.639963   8.192075\n"
     ]
    }
   ],
   "source": [
    "# understanding the data\n",
    "dataset.info()\n",
    "print('\\nData Shape:  {}'.format(dataset.shape))\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccca5e0c",
   "metadata": {},
   "source": [
    "#### Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ef60f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y, yhat):\n",
    "    \"\"\" Pass two arguments\n",
    "    Arguments:\n",
    "        y: responses\n",
    "        yhat: predicted value\n",
    "    Returns:\n",
    "        loss: loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the loss value\n",
    "    loss = np.square(y - yhat).mean()\n",
    "    return loss\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d2558",
   "metadata": {},
   "source": [
    "#### Define function to perform prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef3468e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,x):\n",
    "    \"\"\" Pass two arguments\n",
    "    Arguments:\n",
    "        w: weights\n",
    "        X: input features\n",
    "    Returns:\n",
    "        yhat: predicted values\n",
    "    \"\"\"\n",
    "    # using dot product between 2 matrices to obtain the prediction. \n",
    "    # The prediction is made by multiplying the value of input features \n",
    "    # and their respective estimated weights\n",
    "    yhat = np.dot(w, x.T)\n",
    "    return yhat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743b8158",
   "metadata": {},
   "source": [
    "#### Define function for model training\n",
    "Display the training loss value for each epoch of the training loop. The displayed value must be in 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecdb548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, alpha, max_epoch):\n",
    "    \"\"\" Pass four arguments\n",
    "    Arguments:\n",
    "        X: input features\n",
    "        y: responses\n",
    "        alpha: learning rate\n",
    "        max_epoch: maximum epochs\n",
    "    Returns:\n",
    "        w: estimated weights\n",
    "        hist_loss: training loss history\n",
    "    \"\"\"\n",
    "     #Get the number of samples and number of columns from the shape of input features vector\n",
    "    samples, columns = X.shape\n",
    "    \n",
    "    #Initialise the bias(w0) and the weights\n",
    "    w_init = np.zeros(columns)\n",
    "    w = np.array(w_init)\n",
    "    \n",
    "    hist_loss = []\n",
    "    \n",
    "    #Update the value of weights and bias for every epoch \n",
    "    for i in range (max_epoch):\n",
    "        yhat = predict(w, X)  #predict the response using the previous weight and bias\n",
    "        dw = (1 / samples) * np.dot(X.T, yhat - y)\n",
    "        w = w - (alpha * dw) #update the weights for next epoch\n",
    "        \n",
    "        training_loss = loss_fn(y, yhat)\n",
    "        hist_loss.append(training_loss)\n",
    "        print(\"Epoch %d: Training loss = %.3f\" % (i+1, training_loss))\n",
    "        \n",
    "    return w, hist_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b1e3ae",
   "metadata": {},
   "source": [
    "#### Split the dataset\n",
    "The ratio of training and test is 8:2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54fb47d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all columns except the last one from the dataset\n",
    "X = dataset.iloc[:, :-1]\n",
    "\n",
    "# Select the last column from the dataset\n",
    "y = dataset.iloc[:, -1]\n",
    "\n",
    "# Create a column vector of ones with shape (number of samples, 1)\n",
    "ones_col = np.ones((X.shape[0], 1))\n",
    "\n",
    "# Horizontally stack the column vector of ones with the feature matrix X\n",
    "X = np.hstack((ones_col, X))\n",
    "\n",
    "# split the dataset using train_test_split in 8:2 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77c54d1",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38ad9a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training loss = 1745.552\n",
      "Epoch 2: Training loss = 1392.825\n",
      "Epoch 3: Training loss = 1112.693\n",
      "Epoch 4: Training loss = 890.159\n",
      "Epoch 5: Training loss = 713.333\n",
      "Epoch 6: Training loss = 572.790\n",
      "Epoch 7: Training loss = 461.056\n",
      "Epoch 8: Training loss = 372.200\n",
      "Epoch 9: Training loss = 301.519\n",
      "Epoch 10: Training loss = 245.281\n",
      "Epoch 11: Training loss = 200.521\n",
      "Epoch 12: Training loss = 164.887\n",
      "Epoch 13: Training loss = 136.511\n",
      "Epoch 14: Training loss = 113.908\n",
      "Epoch 15: Training loss = 95.898\n",
      "Epoch 16: Training loss = 81.544\n",
      "Epoch 17: Training loss = 70.100\n",
      "Epoch 18: Training loss = 60.974\n",
      "Epoch 19: Training loss = 53.694\n",
      "Epoch 20: Training loss = 47.886\n",
      "Epoch 21: Training loss = 43.249\n",
      "Epoch 22: Training loss = 39.548\n",
      "Epoch 23: Training loss = 36.592\n",
      "Epoch 24: Training loss = 34.230\n",
      "Epoch 25: Training loss = 32.343\n",
      "Epoch 26: Training loss = 30.835\n",
      "Epoch 27: Training loss = 29.629\n",
      "Epoch 28: Training loss = 28.665\n",
      "Epoch 29: Training loss = 27.893\n",
      "Epoch 30: Training loss = 27.275\n",
      "Epoch 31: Training loss = 26.781\n",
      "Epoch 32: Training loss = 26.385\n",
      "Epoch 33: Training loss = 26.068\n",
      "Epoch 34: Training loss = 25.814\n",
      "Epoch 35: Training loss = 25.611\n",
      "Epoch 36: Training loss = 25.448\n",
      "Epoch 37: Training loss = 25.317\n",
      "Epoch 38: Training loss = 25.212\n",
      "Epoch 39: Training loss = 25.128\n",
      "Epoch 40: Training loss = 25.060\n",
      "Epoch 41: Training loss = 25.006\n",
      "Epoch 42: Training loss = 24.962\n",
      "Epoch 43: Training loss = 24.927\n",
      "Epoch 44: Training loss = 24.899\n",
      "Epoch 45: Training loss = 24.877\n",
      "Epoch 46: Training loss = 24.859\n",
      "Epoch 47: Training loss = 24.844\n",
      "Epoch 48: Training loss = 24.832\n",
      "Epoch 49: Training loss = 24.823\n",
      "Epoch 50: Training loss = 24.815\n",
      "Epoch 51: Training loss = 24.809\n",
      "Epoch 52: Training loss = 24.804\n",
      "Epoch 53: Training loss = 24.800\n",
      "Epoch 54: Training loss = 24.797\n",
      "Epoch 55: Training loss = 24.794\n",
      "Epoch 56: Training loss = 24.792\n",
      "Epoch 57: Training loss = 24.791\n",
      "Epoch 58: Training loss = 24.789\n",
      "Epoch 59: Training loss = 24.788\n",
      "Epoch 60: Training loss = 24.787\n",
      "Epoch 61: Training loss = 24.787\n",
      "Epoch 62: Training loss = 24.786\n",
      "Epoch 63: Training loss = 24.786\n",
      "Epoch 64: Training loss = 24.785\n",
      "Epoch 65: Training loss = 24.785\n",
      "Epoch 66: Training loss = 24.785\n",
      "Epoch 67: Training loss = 24.784\n",
      "Epoch 68: Training loss = 24.784\n",
      "Epoch 69: Training loss = 24.784\n",
      "Epoch 70: Training loss = 24.784\n",
      "Epoch 71: Training loss = 24.784\n",
      "Epoch 72: Training loss = 24.784\n",
      "Epoch 73: Training loss = 24.784\n",
      "Epoch 74: Training loss = 24.784\n",
      "Epoch 75: Training loss = 24.784\n",
      "Epoch 76: Training loss = 24.784\n",
      "Epoch 77: Training loss = 24.784\n",
      "Epoch 78: Training loss = 24.784\n",
      "Epoch 79: Training loss = 24.784\n",
      "Epoch 80: Training loss = 24.784\n",
      "Epoch 81: Training loss = 24.784\n",
      "Epoch 82: Training loss = 24.784\n",
      "Epoch 83: Training loss = 24.784\n",
      "Epoch 84: Training loss = 24.784\n",
      "Epoch 85: Training loss = 24.784\n",
      "Epoch 86: Training loss = 24.784\n",
      "Epoch 87: Training loss = 24.784\n",
      "Epoch 88: Training loss = 24.784\n",
      "Epoch 89: Training loss = 24.784\n",
      "Epoch 90: Training loss = 24.784\n",
      "Epoch 91: Training loss = 24.784\n",
      "Epoch 92: Training loss = 24.784\n",
      "Epoch 93: Training loss = 24.784\n",
      "Epoch 94: Training loss = 24.784\n",
      "Epoch 95: Training loss = 24.784\n",
      "Epoch 96: Training loss = 24.784\n",
      "Epoch 97: Training loss = 24.784\n",
      "Epoch 98: Training loss = 24.784\n",
      "Epoch 99: Training loss = 24.784\n",
      "Epoch 100: Training loss = 24.784\n"
     ]
    }
   ],
   "source": [
    "# Train the model using the training data with a learning rate of 0.1 and maximum of 100 epochs\n",
    "w, hist_loss = train_model(X_train, y_train, 0.1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb404cfd",
   "metadata": {},
   "source": [
    "#### Display the estimated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd8ad6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated weights are:\n",
      " [ 9.62881097e+00  1.16594602e+01 -9.61370460e-03  2.44856425e-01\n",
      "  3.69632698e+01  1.46198298e-01]\n",
      "\n",
      "\n",
      "Weight for x0: 9.628810968980886\n",
      "Weight for x1: 11.659460221011164\n",
      "Weight for x2: -0.009613704595479439\n",
      "Weight for x3: 0.2448564246199856\n",
      "Weight for x4: 36.96326976722024\n",
      "Weight for x5: 0.14619829824401628\n"
     ]
    }
   ],
   "source": [
    "# Print the estimated weights\n",
    "print(\"The estimated weights are:\\n\", w)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Iterate over the weights and print them individually\n",
    "for i in range(len(w)):\n",
    "    print(\"Weight for x{}: {}\".format(i, w[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b09cee",
   "metadata": {},
   "source": [
    "#### Display the training loss against epoch graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6012d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training loss history graph\n",
    "\n",
    "plt.plot(hist_loss)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Training Loss Against Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9fdd0c",
   "metadata": {},
   "source": [
    "#### Predict the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddd24f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test data using the estimated weights\n",
    "y_pred = predict(w, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8c5ae42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Prediction   Response\n",
      "0      2.581473  -0.223583\n",
      "1     55.056507  53.173508\n",
      "2      6.132532  -1.905123\n",
      "3    -45.866341 -43.951402\n",
      "4    -11.147457  -5.041251\n",
      "..          ...        ...\n",
      "195   59.557342  67.416960\n",
      "196   55.028231  53.388698\n",
      "197  -31.300357 -32.018401\n",
      "198   93.948323  85.669029\n",
      "199   32.282950  35.732950\n",
      "\n",
      "[200 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the prediction and response(actual)\n",
    "\n",
    "# Create a table to store the value of prediction and response\n",
    "predictionTable = pd.DataFrame()\n",
    "predictionTable['Prediction'] = pd.Series(y_pred)\n",
    "# use to_numpy to convert y_test from pandas series to an array\n",
    "predictionTable[\"Response\"] = y_test.to_numpy()\n",
    "print(predictionTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609adeb8",
   "metadata": {},
   "source": [
    "#### Display the r2 score, mean squared error and mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e0e1bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score is: 0.9830263224395278\n",
      "The mean squared error is: 26.052842615096115\n",
      "The mean absolute error is: 4.074753534842067\n"
     ]
    }
   ],
   "source": [
    "# Calculate the R-squared score\n",
    "SSE = np.sum(np.square(y_test - y_pred))\n",
    "SST = np.sum(np.square(y_test - y_test.mean()))\n",
    "r2 = 1 - (SSE/SST)\n",
    "print(\"The R2 score is:\", r2)\n",
    "\n",
    "# Calculate the mean squared error (MSE)\n",
    "mse = np.square(y_test - y_pred).mean()\n",
    "print(\"The mean squared error is:\", mse)\n",
    "\n",
    "# Calculate the mean absolute error (MAE)\n",
    "abs_diff = np.abs(y_test - y_pred)\n",
    "mae = np.mean(abs_diff)\n",
    "print(\"The mean absolute error is:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0667409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analysis for test set:\n",
      "\n",
      "R2 Score: 0.9830263224395278\n",
      "Mean squared error: 26.052842615096115\n",
      "Mean absolute error: 4.074753534842067\n"
     ]
    }
   ],
   "source": [
    "print(\"Error analysis for test set:\\n\")\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R2 Score: {}\".format(r2))\n",
    "mse = loss_fn(y_test, y_pred)\n",
    "print(\"Mean squared error: {}\".format(mse))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean absolute error: {}\".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fddb7c",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "The following analysis is made with the assignment1_dataset.csv in mind, using the provided training function, with a training/test split of 8:2, learning rate(alpha) of 0.1 and number of epochs of 100.\n",
    "\n",
    "### R2 score:\n",
    "R2 = 1 - SSE/SST, where SSE is the sum of squared errors between predictions and responses, while SST is the sum of squared errors of the baseline model where the prediction is always made with the mean of responses, y.\n",
    "SSE  \n",
    "\n",
    "The R2 value range from 0 to 1, and a higher value of R2 indicates better performance of the model.  \n",
    "For the test set, since the R2 score is quite high at 0.983, we can say that its R2 score indicates that it is a well-performing model\n",
    "\n",
    "### Mean squared error:\n",
    "Mean squared error is the sum of the squared difference between the prediction(y_hat) and response(actual y).  \n",
    "\n",
    "For the test set, the squared difference between the predicted and actual values of the target variable is 13.439, which is quite low and is similar to the MSE of the training set. This indicates that the model is working well and no overfitting had occurred.  \n",
    "\n",
    "MSE is also used to calculate the loss function, the lowest value(local minima) is reached by the model at 67 epochs with a learning rate(alpha) of 0.1, with MSE = 26.053 and plateuing after that.  \n",
    "\n",
    "A lower value of mean squared error is desirable, through gradient descent, we are able to achieve the minimal loss function value, indicated by the decreasing mean squared error.  \n",
    "\n",
    "\n",
    "### Mean absolute error: \n",
    "Mean absolute error is similar to mean squared error, but instead of squaring the error, we just apply the absolute value function on it, so that the error is always >= 0. (Chugh, 2020)\n",
    "\n",
    "Compared to mean squared error, due to the lack of squaring, mean absolute error punishes large error less.\n",
    "\n",
    "For the test set, the the mean absolute error is 4.075. This means that on average, the predictions will differ from the responses by 4.075. The low mean absolute error indicates that the model performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465df252",
   "metadata": {},
   "source": [
    "## Reference(s):\n",
    "Chugh, A. (2020, December 8). MAE, MSE, RMSE, Coefficient of Determination, Adjusted R Squared — Which Metric is Better? Medium. https://medium.com/analytics-vidhya/mae-mse-rmse-coefficient-of-determination-adjusted-r-squared-which-metric-is-better-cd0326a5697e  \n",
    "\n",
    "Noor M.H.M. (2023). Gradient Descent – Halim Noor. Halim Noor. https://halimnoor.com/gradient-descent/  \n",
    "\n",
    "Noor M.H.M. (2023). Linear Regression – Halim Noor. Halim Noor. https://halimnoor.com/linear-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00a2030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f2b367c7e0b9a76174eeacc9d6a20e0fa950d58d4c857ebaa07d992a63ed5c61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
